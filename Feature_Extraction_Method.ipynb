{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWAGMzwKYeLYyEy8yLzAvb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiha-mahin/Data-Analysis/blob/main/Feature_Extraction_Method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1.Chi-Square (œá¬≤) feature extraction method**\n",
        "\n",
        "### üåü What is Chi-Square Feature Extraction?\n",
        "\n",
        "The **Chi-Square (œá¬≤) test** is a **statistical method** used in **feature selection** to find out whether two variables (usually **feature and target**) are **independent**.\n",
        "\n",
        "In feature selection, we use the Chi-Square test to **rank categorical input features** based on how strongly they are related to the **categorical output (class label)**.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç When to Use:\n",
        "\n",
        "* Input: **Categorical features**\n",
        "* Output (target): **Categorical class label**\n",
        "\n",
        "> ‚ö†Ô∏è If you have **numeric data**, you need to **convert it to categorical** (e.g., using binning).\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Key Idea:\n",
        "\n",
        "Chi-Square score =\n",
        "\n",
        "$$\n",
        "\\chi^2 = \\sum \\frac{(O - E)^2}{E}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $O$: Observed frequency (from the dataset)\n",
        "* $E$: Expected frequency (if there was no relationship)\n",
        "\n",
        "If $œá^2$ is **high**, the feature and label are **dependent** ‚Üí good feature.\n",
        "If $œá^2$ is **low**, the feature and label are **independent** ‚Üí bad feature.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Example\n",
        "\n",
        "Let‚Äôs say we are trying to predict whether a person **buys a phone** (Yes/No) based on **Age Group**.\n",
        "\n",
        "| Age Group | Buys Phone: Yes | Buys Phone: No |\n",
        "| --------- | --------------- | -------------- |\n",
        "| Teen      | 10              | 30             |\n",
        "| Adult     | 30              | 10             |\n",
        "| Senior    | 20              | 20             |\n",
        "\n",
        "---\n",
        "\n",
        "### üßÆ Step-by-step Calculation\n",
        "\n",
        "#### Step 1: Total values\n",
        "\n",
        "* Total people: 10+30+20+30+10+20 = 120\n",
        "* Total Yes: 10+30+20 = 60\n",
        "* Total No: 30+10+20 = 60\n",
        "\n",
        "---\n",
        "\n",
        "#### Step 2: Expected Frequencies (E)\n",
        "\n",
        "Expected = (Row Total √ó Column Total) / Grand Total\n",
        "\n",
        "Example: For \"Teen-Yes\" =\n",
        "\n",
        "$$\n",
        "\\text{Expected} = \\frac{(Teen Total) √ó (Yes Total)}{Grand Total} = \\frac{40 √ó 60}{120} = 20\n",
        "$$\n",
        "\n",
        "Compute for all cells:\n",
        "\n",
        "| Age Group | Yes (O, E) | No (O, E) |\n",
        "| --------- | ---------- | --------- |\n",
        "| Teen      | 10, 20     | 30, 20    |\n",
        "| Adult     | 30, 20     | 10, 20    |\n",
        "| Senior    | 20, 20     | 20, 20    |\n",
        "\n",
        "---\n",
        "\n",
        "#### Step 3: Compute Chi-Square Score\n",
        "\n",
        "Use formula:\n",
        "\n",
        "$$\n",
        "\\chi^2 = \\sum \\frac{(O - E)^2}{E}\n",
        "$$\n",
        "\n",
        "$$\n",
        "œá^2 = \\frac{(10 - 20)^2}{20} + \\frac{(30 - 20)^2}{20} + \\cdots\n",
        "= \\frac{100}{20} + \\frac{100}{20} + 0 + 0 + 0 + 0 = 10 + 10 = 20\n",
        "$$\n",
        "\n",
        "So, **œá¬≤ = 20** (a high score ‚Üí strong relationship).\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### üí° Summary\n",
        "\n",
        "| Aspect                | Chi-Square Test                        |\n",
        "| --------------------- | -------------------------------------- |\n",
        "| Data type required    | Categorical                            |\n",
        "| Purpose               | Feature selection                      |\n",
        "| Measures              | Dependency between feature and label   |\n",
        "| High Chi-Square value | Strong relationship ‚Üí keep the feature |\n",
        "| Low Chi-Square value  | Weak relationship ‚Üí drop the feature   |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PCyRmJSGAygv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pht76hBeAs_J",
        "outputId": "dc77e5e0-b143-434b-affc-1cf59f79a4b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features:\n",
            " [[0 2]\n",
            " [1 0]\n",
            " [1 1]\n",
            " [0 2]]\n",
            "Scores:\n",
            " [1.28571429 2.         1.8       ]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "# Sample dataset (X must be non-negative and categorical or encoded as integers)\n",
        "X = [[1, 0, 2],\n",
        "     [2, 1, 0],\n",
        "     [3, 1, 1],\n",
        "     [1, 0, 2]]\n",
        "\n",
        "y = [0, 1, 1, 0]  # Class label\n",
        "\n",
        "# Apply Chi-Square\n",
        "chi2_selector = SelectKBest(score_func=chi2, k=2)\n",
        "X_kbest = chi2_selector.fit_transform(X, y)\n",
        "\n",
        "print(\"Selected Features:\\n\", X_kbest)\n",
        "print(\"Scores:\\n\", chi2_selector.scores_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2.Mutual Information (MI)**\n",
        "\n",
        "\n",
        "\n",
        "## üß† What is Mutual Information?\n",
        "\n",
        "Mutual Information (MI) tells us **how much information one variable gives us about another**.\n",
        "\n",
        "In feature selection, we use MI to find out:\n",
        "\n",
        "> ‚ùì ‚ÄúDoes this feature tell me anything useful about the class label?‚Äù\n",
        "\n",
        "If yes ‚Üí It‚Äôs a good feature\n",
        "If no ‚Üí It‚Äôs a useless feature\n",
        "\n",
        "---\n",
        "\n",
        "### üåü Think of it Like This:\n",
        "\n",
        "* If knowing feature **X** tells you a lot about the class **Y**, the **mutual information is high**.\n",
        "* If knowing feature **X** gives you no clue about **Y**, the **mutual information is low (or zero)**.\n",
        "\n",
        "---\n",
        "\n",
        "### üßÆ Formula (Just for awareness):\n",
        "\n",
        "$$\n",
        "MI(X, Y) = \\sum_{x,y} P(x,y) \\cdot \\log\\left(\\frac{P(x,y)}{P(x)P(y)}\\right)\n",
        "$$\n",
        "\n",
        "But don't worry ‚Äî in practice, **you don‚Äôt have to calculate this manually**. Libraries like `sklearn` do it for you.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Simple Example:\n",
        "\n",
        "Let‚Äôs say we‚Äôre predicting whether a student **passed** or **failed**, and one of the features is whether they **studied or not**.\n",
        "\n",
        "### Dataset:\n",
        "\n",
        "| Studied | Result |\n",
        "| ------- | ------ |\n",
        "| Yes     | Pass   |\n",
        "| No      | Fail   |\n",
        "| Yes     | Pass   |\n",
        "| No      | Fail   |\n",
        "| Yes     | Pass   |\n",
        "\n",
        "If every time someone studies, they pass ‚Üí **Strong relationship**\n",
        "Mutual Information = **High**\n",
        "\n",
        "Now imagine:\n",
        "\n",
        "| Studied | Result |\n",
        "| ------- | ------ |\n",
        "| Yes     | Pass   |\n",
        "| Yes     | Fail   |\n",
        "| No      | Pass   |\n",
        "| No      | Fail   |\n",
        "\n",
        "Here, studying has no pattern with the result.\n",
        "Mutual Information = **Low (‚âà 0)**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### üìù Summary Table:\n",
        "\n",
        "| Feature Selection Method | Works with             | Handles Non-Linear | Needs Normal Data? | Good for Imbalanced Data? |\n",
        "| ------------------------ | ---------------------- | ------------------ | ------------------ | ------------------------- |\n",
        "| **Mutual Information**   | Categorical or numeric | ‚úÖ Yes              | ‚ùå No               | ‚úÖ Yes (better than chi¬≤)  |\n",
        "| Chi-Square               | Categorical only       | ‚ùå No               | ‚ùå No               | ‚ùå Biased                  |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vS6r6glDH1kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "\n",
        "# Sample dataset\n",
        "X = [[1, 0, 2],\n",
        "     [2, 1, 0],\n",
        "     [3, 1, 1],\n",
        "     [1, 0, 2]]\n",
        "y = [0, 1, 1, 0]\n",
        "\n",
        "# Apply Mutual Information\n",
        "mi = mutual_info_classif(X, y)\n",
        "print(\"Mutual Info Scores:\", mi)\n",
        "\n",
        "# Select top 2 features\n",
        "selector = SelectKBest(score_func=mutual_info_classif, k=2)\n",
        "X_new = selector.fit_transform(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDuq9uzGH0rq",
        "outputId": "bf8cc22d-02bf-4078-fd91-d8e47a36a64d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mutual Info Scores: [0.20833333 0.83333333 0.45833333]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Most Important Feature Extraction/Selection Methods**\n",
        "\n",
        "## **1. For Tabular Data (Structured Data)**\n",
        "\n",
        "| Method                                 | Type                                     | Works On                        | Use When...                                                                                  |\n",
        "| -------------------------------------- | ---------------------------------------- | ------------------------------- | -------------------------------------------------------------------------------------------- |\n",
        "| **Chi-Square Test**                    | Filter Method                            | Categorical                     | You want to test dependence between feature and class (but watch out for imbalance).         |\n",
        "| **Mutual Information**                 | Filter Method                            | Categorical/Discrete/Continuous | You want to capture both linear and non-linear dependencies.                                 |\n",
        "| **ANOVA F-Test**                       | Filter Method                            | Numerical + Categorical Label   | For selecting features with high variance across classes.                                    |\n",
        "| **Correlation Matrix**                 | Filter Method                            | Numerical                       | To remove redundant features (features that are highly correlated with each other).          |\n",
        "| **L1 Regularization (Lasso)**          | Embedded Method                          | Numerical                       | You want to shrink unimportant features to zero directly in model training.                  |\n",
        "| **Tree-based Feature Importance**      | Embedded Method                          | Any (auto-handles types)        | You want fast feature scoring with decision tree-based models (like Random Forest, XGBoost). |\n",
        "| **PCA (Principal Component Analysis)** | Dimensionality Reduction                 | Numerical                       | You want to transform features to a smaller set of *uncorrelated* ones.                      |\n",
        "| **Autoencoders**                       | Dimensionality Reduction (Deep Learning) | Numerical                       | You want to learn new compressed feature representations automatically.                      |\n",
        "\n",
        "## **2. For Text Data (NLP)**\n",
        "\n",
        "| Method               | Type                 | Description                                                                |\n",
        "| -------------------- | -------------------- | -------------------------------------------------------------------------- |\n",
        "| **TF-IDF**           | Extraction           | Calculates how important a word is in a document relative to a collection. |\n",
        "| **Word2Vec / GloVe** | Embedding            | Converts words into dense vectors capturing semantic meaning.              |\n",
        "| **BERT Embeddings**  | Contextual Embedding | Converts full sentences/words to vectors using transformer-based models.   |\n",
        "\n",
        "\n",
        "## **3. For Image Data**\n",
        "\n",
        "| Method                                    | Type                     | Description                                             |\n",
        "| ----------------------------------------- | ------------------------ | ------------------------------------------------------- |\n",
        "| **HOG (Histogram of Oriented Gradients)** | Traditional Extraction   | Good for object detection in classical computer vision. |\n",
        "| **SIFT, SURF**                            | Traditional Extraction   | Keypoint detection, image matching.                     |\n",
        "| **CNN Feature Maps**                      | Deep Learning Extraction | Extract features from intermediate CNN layers.          |\n",
        "\n",
        "\n",
        "#**Which Should You Use?**\n",
        "\n",
        "| Your Case                                        | Best Feature Methods                             |\n",
        "| ------------------------------------------------ | ------------------------------------------------ |\n",
        "| Tabular classification (categorical + numerical) | Mutual Info, Chi-Square, Lasso, Random Forest    |\n",
        "| Imbalanced data                                  | SMOTE + Mutual Info or Tree-Based Importance     |\n",
        "| Too many numerical features                      | PCA or Lasso                                     |\n",
        "| Text classification                              | TF-IDF or BERT embeddings                        |\n",
        "| Images                                           | CNN feature maps (e.g., using pretrained ResNet) |\n",
        "\n"
      ],
      "metadata": {
        "id": "zJU1K7QDJTYK"
      }
    }
  ]
}