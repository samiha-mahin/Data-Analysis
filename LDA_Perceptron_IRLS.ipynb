{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuOZKWqUPzSTtqhwtcG5YN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiha-mahin/Data-Analysis/blob/main/LDA_Perceptron_IRLS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LDA (Linear Discriminant Analysis)**\n",
        "\n",
        "## üß† What is LDA?\n",
        "\n",
        "**LDA** is a **supervised machine learning technique** used for:\n",
        "\n",
        "* **Dimensionality reduction** (like PCA, but supervised)\n",
        "* **Classification** (especially when classes are clearly separated)\n",
        "\n",
        "It finds a new axis (line) that best **separates the classes** in your data.\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Basic Idea\n",
        "\n",
        "Imagine you have data of two types of fruits:\n",
        "\n",
        "* üçé Apples\n",
        "* üçä Oranges\n",
        "\n",
        "Each fruit has 2 features:\n",
        "\n",
        "* **Weight**\n",
        "* **Color Score**\n",
        "\n",
        "You want to reduce it to **1 feature** that still separates apples and oranges well.\n",
        "\n",
        "LDA will:\n",
        "\n",
        "* Find a **new line** (called a discriminant) that **best separates** apples and oranges.\n",
        "* Project all the data points onto this line.\n",
        "* This helps in:\n",
        "\n",
        "  * Better **visualization**\n",
        "  * Faster **training**\n",
        "  * Sometimes better **classification accuracy**\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Simple Example:\n",
        "\n",
        "| Fruit  | Weight (g) | Color Score |\n",
        "| ------ | ---------- | ----------- |\n",
        "| Apple  | 150        | 0.8         |\n",
        "| Apple  | 160        | 0.75        |\n",
        "| Orange | 170        | 0.3         |\n",
        "| Orange | 180        | 0.25        |\n",
        "\n",
        "Here:\n",
        "\n",
        "* Apples are **heavier & redder**\n",
        "* Oranges are **heavier & orange**\n",
        "\n",
        "LDA will combine Weight and Color Score into a **single feature** that best separates the two classes.\n",
        "\n",
        "---\n",
        "\n",
        "## üìà LDA vs PCA\n",
        "\n",
        "| Feature     | LDA                           | PCA                            |\n",
        "| ----------- | ----------------------------- | ------------------------------ |\n",
        "| Supervised? | ‚úÖ Yes (uses class labels)     | ‚ùå No (ignores class labels)    |\n",
        "| Goal        | Maximize **class separation** | Maximize **data spread**       |\n",
        "| Better for‚Ä¶ | Classification                | Visualization, noise reduction |\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Python Code Example:\n",
        "\n",
        "```python\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "X = pd.DataFrame({\n",
        "    'Weight': [150, 160, 170, 180],\n",
        "    'ColorScore': [0.8, 0.75, 0.3, 0.25]\n",
        "})\n",
        "y = ['Apple', 'Apple', 'Orange', 'Orange']\n",
        "\n",
        "# Apply LDA\n",
        "lda = LinearDiscriminantAnalysis(n_components=1)\n",
        "X_lda = lda.fit_transform(X, y)\n",
        "\n",
        "# Show result\n",
        "print(X_lda)\n",
        "```\n",
        "\n",
        "This will give you a new 1D feature that keeps Apple and Orange points **far apart** on a line.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ú® Summary\n",
        "\n",
        "* LDA is used to **separate classes** and **reduce dimensions**.\n",
        "* It's **supervised**, uses labels.\n",
        "* Useful when you want to **simplify data** but still **keep class differences**.\n",
        "* Common in **face recognition**, **text classification**, etc.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "UsjOF8lehB9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perceptron (Error Correction Learning)**\n",
        "\n",
        "## üåü What is Perceptron?\n",
        "\n",
        "A **Perceptron** is the **simplest kind of neural network** ‚Äî just one neuron!\n",
        "\n",
        "It tries to:\n",
        "\n",
        "* **Draw a line** between two classes\n",
        "* **Learn** by checking if its prediction is wrong\n",
        "* **Correct its mistake** by changing the weights (error correction)\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Perceptron Formula\n",
        "\n",
        "For input $x = [x_1, x_2]$, weights $w = [w_1, w_2]$, and bias $b$:\n",
        "\n",
        "```\n",
        "prediction = 1     if (w1*x1 + w2*x2 + b) > 0  \n",
        "           = 0     otherwise\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Learning Rule (Error Correction)\n",
        "\n",
        "If prediction is **wrong**, then update:\n",
        "\n",
        "```\n",
        "w = w + learning_rate √ó (true_label - predicted) √ó x\n",
        "b = b + learning_rate √ó (true_label - predicted)\n",
        "```\n",
        "\n",
        "This is called **error correction learning**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Example Step-by-Step\n",
        "\n",
        "Let‚Äôs classify this toy dataset:\n",
        "\n",
        "| x1 | x2 | label |\n",
        "| -- | -- | ----- |\n",
        "| 2  | 3  | 1     |\n",
        "| 1  | 1  | 0     |\n",
        "\n",
        "### Step 1: Start with weights and bias\n",
        "\n",
        "```\n",
        "w = [0, 0], b = 0\n",
        "learning_rate = 1\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: First data point (2, 3), label = 1\n",
        "\n",
        "```\n",
        "prediction = 0 (because w‚ãÖx + b = 0)\n",
        "```\n",
        "\n",
        "It‚Äôs **wrong** ‚Üí update:\n",
        "\n",
        "```\n",
        "w = [0, 0] + 1*(1 - 0)*[2, 3] = [2, 3]\n",
        "b = 0 + 1*(1 - 0) = 1\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: Second point (1, 1), label = 0\n",
        "\n",
        "Now:\n",
        "\n",
        "```\n",
        "prediction = 1 (since 2*1 + 3*1 + 1 = 6)\n",
        "```\n",
        "\n",
        "It‚Äôs **wrong again** ‚Üí update:\n",
        "\n",
        "```\n",
        "w = [2, 3] + 1*(0 - 1)*[1, 1] = [1, 2]\n",
        "b = 1 + 1*(0 - 1) = 0\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Over time:\n",
        "\n",
        "The perceptron keeps adjusting its **weights and bias** until it finds a line that **correctly separates** the two classes (if linearly separable).\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Visualization Idea\n",
        "\n",
        "Imagine it like this:\n",
        "\n",
        "```\n",
        "Class 0: ‚óè (blue)\n",
        "Class 1: ‚ñ≤ (red)\n",
        "\n",
        "The perceptron moves the decision line each time it makes a mistake.\n",
        "```\n",
        "\n",
        "Eventually, the line **rotates and shifts** until all blue points are on one side, red on the other.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Limitations\n",
        "\n",
        "* **Only works** if the data is **linearly separable** (can draw a straight line between classes)\n",
        "* Can get stuck if data is **overlapping** or **noisy**\n",
        "\n",
        "---\n",
        "\n",
        "## üèÅ Summary\n",
        "\n",
        "| Feature       | Description                                |\n",
        "| ------------- | ------------------------------------------ |\n",
        "| Purpose       | Classify two classes using a straight line |\n",
        "| Learning Type | Error-based (supervised)                   |\n",
        "| Update Rule   | Adjust weights when prediction is wrong    |\n",
        "| Good For      | Simple, linearly separable data            |\n",
        "| Not Good For  | Overlapping, nonlinear problems            |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wYcCYxCqkqjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IRLS (Iteratively Reweighted Least Squares)**\n",
        "\n",
        "\n",
        "\n",
        "## üåü What is IRLS?\n",
        "\n",
        "IRLS is a method used to **fit models like Logistic Regression** ‚Äî where you **can't just solve with a simple equation**.\n",
        "\n",
        "Instead of solving once, it:\n",
        "\n",
        "1. Starts with a guess\n",
        "2. Solves a weighted least squares problem\n",
        "3. Updates the weights based on how confident it is\n",
        "4. Repeats until the model gets better\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why do we need IRLS for Logistic Regression?\n",
        "\n",
        "Logistic Regression predicts **probabilities** using a sigmoid function:\n",
        "\n",
        "$$\n",
        "P(y=1 \\mid x) = \\frac{1}{1 + e^{-(w \\cdot x + b)}}\n",
        "$$\n",
        "\n",
        "Unlike linear regression, **you can‚Äôt just solve it in one step**.\n",
        "So IRLS helps us solve it by approximating it **step-by-step using weighted linear regression**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ IRLS Intuition in Simple Steps:\n",
        "\n",
        "1. Start with **initial weights** $w$\n",
        "2. Predict **probabilities** for all data points\n",
        "3. Calculate the **weights** (confidence) for each data point\n",
        "4. Solve a weighted linear regression (like normal equation)\n",
        "5. Update the weights $w$\n",
        "6. Repeat until convergence\n",
        "\n",
        "---\n",
        "\n",
        "## üìò Example (Simple)\n",
        "\n",
        "Imagine this tiny dataset:\n",
        "\n",
        "| x | y |\n",
        "| - | - |\n",
        "| 1 | 0 |\n",
        "| 2 | 0 |\n",
        "| 3 | 1 |\n",
        "| 4 | 1 |\n",
        "\n",
        "We want to fit Logistic Regression using IRLS.\n",
        "\n",
        "### Step-by-step (simplified):\n",
        "\n",
        "#### 1. Initialize $w = 0$\n",
        "\n",
        "#### 2. Predict:\n",
        "\n",
        "$$\n",
        "P(y=1) = \\frac{1}{1 + e^{-wx}} = 0.5 \\text{ for all } x \\text{ since } w = 0\n",
        "$$\n",
        "\n",
        "#### 3. Calculate weight matrix $W$ using:\n",
        "\n",
        "$$\n",
        "W_i = P(y=1)_i \\times (1 - P(y=1)_i)\n",
        "$$\n",
        "\n",
        "Here, all $W_i = 0.25$\n",
        "\n",
        "#### 4. Solve:\n",
        "\n",
        "Weighted least squares:\n",
        "\n",
        "$$\n",
        "(X^T W X) w = X^T W z\n",
        "$$\n",
        "\n",
        "Where $z = Xw + \\frac{(y - p)}{W}$ ‚Üí a kind of corrected output\n",
        "\n",
        "#### 5. Update $w$, repeat steps 2‚Äì5 until weights stop changing.\n",
        "\n",
        "---\n",
        "\n",
        "## üñºÔ∏è Visual Example\n",
        "\n",
        "Imagine plotting points like this:\n",
        "\n",
        "* (1, 0), (2, 0), (3, 1), (4, 1)\n",
        "\n",
        "IRLS adjusts the logistic curve until it bends through the middle of the 0s and 1s.\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Real-life Analogy\n",
        "\n",
        "Think of IRLS like this:\n",
        "\n",
        "* You‚Äôre trying to **draw a curve** between two groups.\n",
        "* You start rough, then you **pay more attention to the data points you‚Äôre uncertain about**.\n",
        "* You repeat this until the curve fits nicely.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Summary Table\n",
        "\n",
        "| Item              | Explanation                                    |\n",
        "| ----------------- | ---------------------------------------------- |\n",
        "| IRLS Full Form    | Iteratively Reweighted Least Squares           |\n",
        "| Used In           | Logistic Regression, Generalized Linear Models |\n",
        "| Idea              | Solve weighted linear regressions repeatedly   |\n",
        "| Weight Importance | High confidence = low weight change            |\n",
        "| Output            | Model parameters that best fit probability     |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EomhMcSclY_z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23BPNdlagari"
      },
      "outputs": [],
      "source": []
    }
  ]
}