{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObKFHEVOv8IhSUEeZCKCMT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiha-mahin/Data-Analysis/blob/main/Imbalanced_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Imbalanced Data in Machine Learning?\n",
        "\n",
        "**Imbalanced data** refers to a classification problem where the classes (or categories) are **not represented equally**. One class has **significantly more samples** than the other(s). This is common in many real-world problems, especially when the **event of interest is rare**.\n",
        "\n",
        "---\n",
        "\n",
        "### Why It‚Äôs a Problem\n",
        "\n",
        "Most machine learning models assume a **roughly equal distribution** of classes. When one class dominates, the model can become **biased** towards the majority class and ignore the minority class, leading to poor performance, especially in detecting the rare class (which is often the more important one).\n",
        "\n",
        "---\n",
        "\n",
        "### Example 1: Fraud Detection\n",
        "\n",
        "| Transaction | Fraud (1) | Not Fraud (0) |\n",
        "| ----------- | --------- | ------------- |\n",
        "| Count       | 100       | 9900          |\n",
        "\n",
        "Here, only 1% of transactions are fraudulent. A naive model could predict **\"Not Fraud\" for every transaction** and still achieve **99% accuracy**, but it would **completely fail** to detect actual fraud cases.\n",
        "\n",
        "---\n",
        "\n",
        "### Example 2: Disease Diagnosis\n",
        "\n",
        "| Patient | Disease Present (1) | Healthy (0) |\n",
        "| ------- | ------------------- | ----------- |\n",
        "| Count   | 200                 | 9800        |\n",
        "\n",
        "The disease is rare. Even if a model has 98% accuracy by just predicting \"Healthy,\" it would be **useless in real-world medical decision-making**, because it misses the diseased patients.\n",
        "\n",
        "---\n",
        "\n",
        "### Consequences of Imbalanced Data\n",
        "\n",
        "* **Misleading accuracy**: High accuracy doesn't mean the model is good.\n",
        "* **Poor recall for the minority class**: The model may miss important cases.\n",
        "* **Unreliable predictions**: Especially in high-risk domains like healthcare or security.\n",
        "\n",
        "---\n",
        "\n",
        "### Metrics to Use Instead of Accuracy\n",
        "\n",
        "In imbalanced settings, use these metrics:\n",
        "\n",
        "* **Precision**: How many predicted positives are actually positive?\n",
        "* **Recall (Sensitivity)**: How many actual positives did we detect?\n",
        "* **F1 Score**: Harmonic mean of precision and recall.\n",
        "* **ROC-AUC / PR-AUC**: Useful for imbalanced classification performance.\n",
        "\n",
        "---\n",
        "\n",
        "### Ways to Handle Imbalanced Data\n",
        "\n",
        "1. **Resampling Methods**\n",
        "\n",
        "   * **Oversampling**: Duplicate or synthetically generate minority class samples (e.g., SMOTE).\n",
        "   * **Undersampling**: Remove some majority class samples.\n",
        "\n",
        "2. **Class Weights**\n",
        "\n",
        "   * Assign more importance to the minority class during training.\n",
        "\n",
        "3. **Anomaly Detection Models**\n",
        "\n",
        "   * Treat the rare class as an anomaly and use specialized models.\n",
        "\n",
        "4. **Ensemble Methods**\n",
        "\n",
        "   * Techniques like Random Forest or XGBoost handle imbalance better with parameter tuning.\n",
        "\n",
        "5. **Change the Evaluation Metric**\n",
        "\n",
        "   * Focus on precision, recall, or F1 instead of accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### Visual Example\n",
        "\n",
        "If you plot the dataset:\n",
        "\n",
        "```plaintext\n",
        "Class 0 (Majority): ooooooooooooooooooooooooooooooo\n",
        "Class 1 (Minority): x x x\n",
        "```\n",
        "\n",
        "A naive classifier might just learn to output ‚Äúo‚Äù every time.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BYzTeFk8m5up"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üîÅ Resampling Techniques for Imbalanced Data\n",
        "\n",
        "In imbalanced classification, **resampling** helps balance the dataset by modifying the class distribution. The two main types are:\n",
        "\n",
        "---\n",
        "\n",
        "## 1. ‚öñÔ∏è **Undersampling**\n",
        "\n",
        "### üîç What It Does:\n",
        "\n",
        "Reduces the number of samples in the **majority class** to match the minority class.\n",
        "\n",
        "### ‚úÖ Pros:\n",
        "\n",
        "* Faster training\n",
        "* Simple to implement\n",
        "\n",
        "### ‚ùå Cons:\n",
        "\n",
        "* Risk of **losing important data**\n",
        "* Might **underfit** the model\n",
        "\n",
        "### üìä Example:\n",
        "\n",
        "```plaintext\n",
        "Original Data:\n",
        "- Class 0 (Not Fraud): 9500 samples\n",
        "- Class 1 (Fraud):     500 samples\n",
        "\n",
        "After Undersampling:\n",
        "- Class 0: 500 samples (randomly selected)\n",
        "- Class 1: 500 samples\n",
        "```\n",
        "\n",
        "Now both classes are balanced with 500 each.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. üîÅ **Oversampling**\n",
        "\n",
        "### üîç What It Does:\n",
        "\n",
        "Increases the number of samples in the **minority class** by duplicating existing samples or generating new ones.\n",
        "\n",
        "### ‚úÖ Pros:\n",
        "\n",
        "* No information is lost from the majority class\n",
        "* Easy to implement\n",
        "\n",
        "### ‚ùå Cons:\n",
        "\n",
        "* Can lead to **overfitting** if the same samples are repeated\n",
        "\n",
        "### üìä Example:\n",
        "\n",
        "```plaintext\n",
        "Original Data:\n",
        "- Class 0: 9500 samples\n",
        "- Class 1: 500 samples\n",
        "\n",
        "After Oversampling:\n",
        "- Class 0: 9500 samples\n",
        "- Class 1: 9500 samples (by duplicating minority class samples)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 3. üß† **SMOTE (Synthetic Minority Over-sampling Technique)**\n",
        "\n",
        "### üîç What It Does:\n",
        "\n",
        "Instead of just duplicating, **SMOTE generates synthetic data** points for the minority class based on the nearest neighbors.\n",
        "\n",
        "### ‚úÖ Pros:\n",
        "\n",
        "* Better than simple oversampling\n",
        "* Reduces overfitting\n",
        "* Makes the minority class more diverse\n",
        "\n",
        "### ‚ùå Cons:\n",
        "\n",
        "* Can create **ambiguous synthetic samples** if classes overlap\n",
        "* More complex and slower\n",
        "\n",
        "### üìä Example:\n",
        "\n",
        "Suppose the minority class has these 2D data points:\n",
        "\n",
        "```\n",
        "[2.0, 3.0]\n",
        "[2.1, 3.2]\n",
        "[1.9, 2.9]\n",
        "```\n",
        "\n",
        "SMOTE will pick a point and a neighbor (e.g., `[2.0, 3.0]` and `[2.1, 3.2]`), then generate a new point in between, like:\n",
        "\n",
        "```\n",
        "New point = [2.05, 3.1]\n",
        "```\n",
        "\n",
        "This point is **not duplicated**, but **synthetically created** based on the feature space.\n",
        "\n",
        "---\n",
        "\n",
        "## üîß When to Use What?\n",
        "\n",
        "| Method        | Best When                                                              |\n",
        "| ------------- | ---------------------------------------------------------------------- |\n",
        "| Undersampling | You have **a lot** of data and can afford to discard some              |\n",
        "| Oversampling  | You have **less data**, and don‚Äôt want to lose any samples             |\n",
        "| SMOTE         | You want to create **realistic** synthetic data for the minority class |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nkA7SkrPqZg3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ROC** and **AUC**\n",
        "\n",
        "## üîç What is ROC?\n",
        "\n",
        "### **ROC** stands for **Receiver Operating Characteristic** curve.\n",
        "\n",
        "It is a **graph** that shows the performance of a classification model at all **classification thresholds**.\n",
        "\n",
        "---\n",
        "\n",
        "### üìà ROC Curve Plots:\n",
        "\n",
        "* **Y-axis:** **True Positive Rate (TPR)** = Recall = Sensitivity\n",
        "\n",
        "  $$\n",
        "  \\text{TPR} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\n",
        "  $$\n",
        "\n",
        "* **X-axis:** **False Positive Rate (FPR)**\n",
        "\n",
        "  $$\n",
        "  \\text{FPR} = \\frac{\\text{False Positives}}{\\text{False Positives + True Negatives}}\n",
        "  $$\n",
        "\n",
        "---\n",
        "\n",
        "## üí° What is AUC?\n",
        "\n",
        "### **AUC** = **Area Under the ROC Curve**\n",
        "\n",
        "* It‚Äôs a **single number** summary of the ROC curve.\n",
        "* **Higher AUC** means **better** model performance.\n",
        "* AUC ranges from **0 to 1**:\n",
        "\n",
        "  * **1.0** = perfect classifier\n",
        "  * **0.5** = no skill (random guess)\n",
        "  * **< 0.5** = worse than guessing\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Example: Binary Classification (Cancer Detection)\n",
        "\n",
        "Let‚Äôs say we built a cancer classifier.\n",
        "\n",
        "| Patient | Actual | Predicted Probability (Cancer) |\n",
        "| ------- | ------ | ------------------------------ |\n",
        "| A       | 1      | 0.95                           |\n",
        "| B       | 0      | 0.90                           |\n",
        "| C       | 1      | 0.80                           |\n",
        "| D       | 0      | 0.60                           |\n",
        "| E       | 0      | 0.40                           |\n",
        "| F       | 1      | 0.30                           |\n",
        "| G       | 0      | 0.10                           |\n",
        "\n",
        "Now, if we try different thresholds like 0.9, 0.7, 0.5, etc., we‚Äôll get different True Positives and False Positives. For each threshold, we calculate:\n",
        "\n",
        "* TPR (Recall)\n",
        "* FPR\n",
        "\n",
        "Plot these points on a graph ‚Üí that‚Äôs your **ROC curve**.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Sample Points on ROC:\n",
        "\n",
        "| Threshold | TPR (Recall) | FPR |\n",
        "| --------- | ------------ | --- |\n",
        "| 0.9       | 1/3          | 1/4 |\n",
        "| 0.7       | 2/3          | 1/4 |\n",
        "| 0.5       | 2/3          | 2/4 |\n",
        "| 0.3       | 3/3          | 3/4 |\n",
        "\n",
        "Plot (FPR, TPR) = (0.25, 0.33), (0.25, 0.66), (0.5, 0.66), etc.\n",
        "\n",
        "Then, the **area under this curve** = **AUC**\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Visual Summary\n",
        "\n",
        "Imagine this:\n",
        "\n",
        "```plaintext\n",
        "Perfect ROC Curve:\n",
        "      |\n",
        "   1  |          ‚óè----------\n",
        "      |         /\n",
        "TPR   |        /\n",
        "      |       /\n",
        "      |      /\n",
        "      |     /\n",
        "   0  |----------------------\n",
        "      0      FPR       1\n",
        "```\n",
        "\n",
        "A model that performs better will **curve closer to the top-left**.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Interpretation:\n",
        "\n",
        "| AUC Value | Meaning                    |\n",
        "| --------- | -------------------------- |\n",
        "| 0.90‚Äì1.0  | Excellent model            |\n",
        "| 0.80‚Äì0.90 | Very good                  |\n",
        "| 0.70‚Äì0.80 | Good/fair                  |\n",
        "| 0.60‚Äì0.70 | Poor                       |\n",
        "| 0.50      | Random (no discrimination) |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J94asbs5rUC9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfVp77QMlruL"
      },
      "outputs": [],
      "source": []
    }
  ]
}