{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtgZmrDpuwsNiKK+sZ1q+E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiha-mahin/Data-Analysis/blob/main/Imbalanced_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Imbalanced Data in Machine Learning?\n",
        "\n",
        "**Imbalanced data** refers to a classification problem where the classes (or categories) are **not represented equally**. One class has **significantly more samples** than the other(s). This is common in many real-world problems, especially when the **event of interest is rare**.\n",
        "\n",
        "---\n",
        "\n",
        "### Why It‚Äôs a Problem\n",
        "\n",
        "Most machine learning models assume a **roughly equal distribution** of classes. When one class dominates, the model can become **biased** towards the majority class and ignore the minority class, leading to poor performance, especially in detecting the rare class (which is often the more important one).\n",
        "\n",
        "---\n",
        "\n",
        "### Example 1: Fraud Detection\n",
        "\n",
        "| Transaction | Fraud (1) | Not Fraud (0) |\n",
        "| ----------- | --------- | ------------- |\n",
        "| Count       | 100       | 9900          |\n",
        "\n",
        "Here, only 1% of transactions are fraudulent. A naive model could predict **\"Not Fraud\" for every transaction** and still achieve **99% accuracy**, but it would **completely fail** to detect actual fraud cases.\n",
        "\n",
        "---\n",
        "\n",
        "### Example 2: Disease Diagnosis\n",
        "\n",
        "| Patient | Disease Present (1) | Healthy (0) |\n",
        "| ------- | ------------------- | ----------- |\n",
        "| Count   | 200                 | 9800        |\n",
        "\n",
        "The disease is rare. Even if a model has 98% accuracy by just predicting \"Healthy,\" it would be **useless in real-world medical decision-making**, because it misses the diseased patients.\n",
        "\n",
        "---\n",
        "\n",
        "### Consequences of Imbalanced Data\n",
        "\n",
        "* **Misleading accuracy**: High accuracy doesn't mean the model is good.\n",
        "* **Poor recall for the minority class**: The model may miss important cases.\n",
        "* **Unreliable predictions**: Especially in high-risk domains like healthcare or security.\n",
        "\n",
        "---\n",
        "\n",
        "### Metrics to Use Instead of Accuracy\n",
        "\n",
        "In imbalanced settings, use these metrics:\n",
        "\n",
        "* **Precision**: How many predicted positives are actually positive?\n",
        "* **Recall (Sensitivity)**: How many actual positives did we detect?\n",
        "* **F1 Score**: Harmonic mean of precision and recall.\n",
        "* **ROC-AUC / PR-AUC**: Useful for imbalanced classification performance.\n",
        "\n",
        "---\n",
        "\n",
        "### Ways to Handle Imbalanced Data\n",
        "\n",
        "1. **Resampling Methods**\n",
        "\n",
        "   * **Oversampling**: Duplicate or synthetically generate minority class samples (e.g., SMOTE).\n",
        "   * **Undersampling**: Remove some majority class samples.\n",
        "\n",
        "2. **Class Weights**\n",
        "\n",
        "   * Assign more importance to the minority class during training.\n",
        "\n",
        "3. **Anomaly Detection Models**\n",
        "\n",
        "   * Treat the rare class as an anomaly and use specialized models.\n",
        "\n",
        "4. **Ensemble Methods**\n",
        "\n",
        "   * Techniques like Random Forest or XGBoost handle imbalance better with parameter tuning.\n",
        "\n",
        "5. **Change the Evaluation Metric**\n",
        "\n",
        "   * Focus on precision, recall, or F1 instead of accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### Visual Example\n",
        "\n",
        "If you plot the dataset:\n",
        "\n",
        "```plaintext\n",
        "Class 0 (Majority): ooooooooooooooooooooooooooooooo\n",
        "Class 1 (Minority): x x x\n",
        "```\n",
        "\n",
        "A naive classifier might just learn to output ‚Äúo‚Äù every time.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BYzTeFk8m5up"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üîÅ Resampling Techniques for Imbalanced Data\n",
        "\n",
        "In imbalanced classification, **resampling** helps balance the dataset by modifying the class distribution. The two main types are:\n",
        "\n",
        "---\n",
        "\n",
        "## 1. ‚öñÔ∏è **Undersampling**\n",
        "\n",
        "### üîç What It Does:\n",
        "\n",
        "Reduces the number of samples in the **majority class** to match the minority class.\n",
        "\n",
        "### ‚úÖ Pros:\n",
        "\n",
        "* Faster training\n",
        "* Simple to implement\n",
        "\n",
        "### ‚ùå Cons:\n",
        "\n",
        "* Risk of **losing important data**\n",
        "* Might **underfit** the model\n",
        "\n",
        "### üìä Example:\n",
        "\n",
        "```plaintext\n",
        "Original Data:\n",
        "- Class 0 (Not Fraud): 9500 samples\n",
        "- Class 1 (Fraud):     500 samples\n",
        "\n",
        "After Undersampling:\n",
        "- Class 0: 500 samples (randomly selected)\n",
        "- Class 1: 500 samples\n",
        "```\n",
        "\n",
        "Now both classes are balanced with 500 each.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. üîÅ **Oversampling**\n",
        "\n",
        "### üîç What It Does:\n",
        "\n",
        "Increases the number of samples in the **minority class** by duplicating existing samples or generating new ones.\n",
        "\n",
        "### ‚úÖ Pros:\n",
        "\n",
        "* No information is lost from the majority class\n",
        "* Easy to implement\n",
        "\n",
        "### ‚ùå Cons:\n",
        "\n",
        "* Can lead to **overfitting** if the same samples are repeated\n",
        "\n",
        "### üìä Example:\n",
        "\n",
        "```plaintext\n",
        "Original Data:\n",
        "- Class 0: 9500 samples\n",
        "- Class 1: 500 samples\n",
        "\n",
        "After Oversampling:\n",
        "- Class 0: 9500 samples\n",
        "- Class 1: 9500 samples (by duplicating minority class samples)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 3. üß† **SMOTE (Synthetic Minority Over-sampling Technique)**\n",
        "\n",
        "### üîç What It Does:\n",
        "\n",
        "Instead of just duplicating, **SMOTE generates synthetic data** points for the minority class based on the nearest neighbors.\n",
        "\n",
        "### ‚úÖ Pros:\n",
        "\n",
        "* Better than simple oversampling\n",
        "* Reduces overfitting\n",
        "* Makes the minority class more diverse\n",
        "\n",
        "### ‚ùå Cons:\n",
        "\n",
        "* Can create **ambiguous synthetic samples** if classes overlap\n",
        "* More complex and slower\n",
        "\n",
        "### üìä Example:\n",
        "\n",
        "Suppose the minority class has these 2D data points:\n",
        "\n",
        "```\n",
        "[2.0, 3.0]\n",
        "[2.1, 3.2]\n",
        "[1.9, 2.9]\n",
        "```\n",
        "\n",
        "SMOTE will pick a point and a neighbor (e.g., `[2.0, 3.0]` and `[2.1, 3.2]`), then generate a new point in between, like:\n",
        "\n",
        "```\n",
        "New point = [2.05, 3.1]\n",
        "```\n",
        "\n",
        "This point is **not duplicated**, but **synthetically created** based on the feature space.\n",
        "\n",
        "---\n",
        "\n",
        "## üîß When to Use What?\n",
        "\n",
        "| Method        | Best When                                                              |\n",
        "| ------------- | ---------------------------------------------------------------------- |\n",
        "| Undersampling | You have **a lot** of data and can afford to discard some              |\n",
        "| Oversampling  | You have **less data**, and don‚Äôt want to lose any samples             |\n",
        "| SMOTE         | You want to create **realistic** synthetic data for the minority class |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nkA7SkrPqZg3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfVp77QMlruL"
      },
      "outputs": [],
      "source": []
    }
  ]
}